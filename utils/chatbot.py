import os
import logging
from dotenv import load_dotenv
import google.generativeai as genai
from datetime import date
import json

# Configure logging
logging.basicConfig(
    level=logging.INFO,  # Use DEBUG for more details
    format="%(asctime)s - %(levelname)s - %(message)s"
)

logging.info("Loading environment variables...")
load_dotenv()

def check_token_limit(input_tokens = 0, output_tokens = 0):
    
    # 1. Read existing data (if the file is empty or missing, initialize an empty dict)
    try:
        with open("../data/tokens_records.json", "r", encoding="utf-8") as f:
            tokens_data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        tokens_data = {}

    # 2. Initialize today's entry if it does not exist yet
    today = str(date.today())
    if today not in tokens_data:
        tokens_data[today] = {"input_tokens": 0, "output_tokens": 0}

    # 3. Update values with new token counts
    tokens_data[today]["input_tokens"] += input_tokens
    tokens_data[today]["output_tokens"] += output_tokens

    # 4. Save updated data back to the JSON file
    with open("../data/tokens_records.json", "w", encoding="utf-8") as f:
        json.dump(tokens_data, f, indent=4, ensure_ascii=False)

    # 5. Log the updated values for debugging/monitoring
    logging.info(
        f"Input tokens used: {tokens_data[today]['input_tokens']}, "
        f"Output tokens used: {tokens_data[today]['output_tokens']}"
    )

    if tokens_data[str(date.today())]["input_tokens"] > 600 or tokens_data[str(date.today())]["output_tokens"] > 600:
        logging.warning("Token limit exceeded for today.")
        return False
    
    else:
        return True


def professor_oak(prompt, item):
    """
    Simulates Professor Oak answering Pokémon-related questions using a Gemini model.

    Args:
        prompt (str): The main user prompt/question.
        item (str): Additional focus/context for the response.

    Returns:
        str: Response generated by the Gemini model as Professor Oak.
    """   

    # Configure Gemini with the API key
    api_key = os.getenv("professor_oak_token")
    if not api_key:
        logging.error("API key not found. Please check your .env file.")
        raise ValueError("Missing API key for Professor Oak.")
    genai.configure(api_key=api_key)
    logging.info("Gemini API configured successfully.")

    # Create the model
    model = genai.GenerativeModel("gemini-2.5-flash")
    logging.debug("Gemini model initialized.")

    # Professor Oak roleplay context
    context = """You are Professor Oak, the renowned Pokémon researcher from Pallet Town.
    You are an expert in Pokémon species, types, evolutions, abilities, history, regions, trainers, and battles.
    Your tone is warm, enthusiastic, and wise—like a mentor guiding young trainers.
    Be as brief and concise as possible.
    Only answer Pokémon-related questions. If asked something else, reply:
    "I'm sorry, young trainer. That topic falls outside my field of Pokémon research."
    """

    # Start a new chat and inject context as an initial user message
    chat = model.start_chat(history=[
        {"role": "user", "parts": [context]}
    ])
    logging.info("Professor Oak chat initialized with roleplay context.")

    # Build the final prompt
    full_prompt = prompt + " focus on " + item
    logging.debug(f"Full prompt: {full_prompt}")

    # Send the message and get the response
    try:

        if check_token_limit():
            response = chat.send_message(full_prompt)
            logging.info("Response successfully generated by Professor Oak.")

            # Token usage (Gemini API)
            input_tokens = response.usage_metadata.prompt_token_count
            output_tokens = response.usage_metadata.candidates_token_count

            check_token_limit(input_tokens, output_tokens)

            answer = response.text

        else:
            answer = "I'm sorry, young trainer. I've reached my research limit for today. Please come back tomorrow!"

    except Exception as e:
        logging.error(f"Error generating response: {e}")
        raise

    return answer
